# 2.6 Forma vectorial de regresión lineal

## Notas

La fórmula de regresión lineal se puede sintetizar con el producto escalar entre las características y los pesos. El vector de características incluye el término de *sesgo* con un valor *x* de uno, como $w_{0}^{x_{i0}},\ donde\ x_{i0} = 1\ para\ w_0$.

Cuando se incluyen todos los registros, la regresión lineal se puede calcular con el producto escalar entre la ***matriz de características*** y el ***vector de pesos***, obteniendo el vector `y` de predicciones.

es decir, que el producto escalar entre un peso y una caracterísca lo representamos así


$g(x_i) = w_0 + \displaystyle\sum_{j=1}^{n} w_j \cdot x_{ij}$


y cuando queremos representar el producto escalar entre la ***matriz de características*** y el ***vector de pesos*** lo hacemos así


$g(x_i) = w_0 + x_{i}^{T}\cdot w$


entonces generalizemos el producto escalar en su forma vectorial de regresión lineal

primero creamos una función que haga el producto escalar(dot product)


```python
def dot(xi, w):
    n = len(xi)

    res = 0.0

    for j in range(n)
    res = res + xi[j] * w[j]
```

ahora podemos ajustar el códifo de nuestro modelo


```python
def linear_regression(xi):
    return w0 + dot(xi, w)
```

recordamos que también es posible hacer lo anterior utilizando la función de Numpy .dot()

```python
def linear_regression(xi):
    return w0 + xi.dot(w)
```

Es posible hacelor de una manera mas corta.

El término de **sesgo** `w0` está fuera del prodcuto escalar, intentaremos integrarlo.

Vamos a imaginar que existe una característica extra en cada auto que la llamaremos $x_{i0}$ que siempre valdrá 1 y que por lo tanto no afectará nada


$g(x_i) = w_0\cdot x_{i0}  + x_{i}^{T}\cdot w$


por tanto tenemos el vector de pesos(weights) de dimensión n+1  que tiene el peso de la caracterísitica extra $w_{0}$


$w = [w_{0}, w_{1}, w_{2},..., w_{n}]$


y también tenemos el vector de características donde $x_{i0}$ siempre valdrá 1


$x_{i} = [x_{i0}, x_{i1}, x_{i2},..., w_{in}]$


por tanto cuando hacemos el prodcuto escalar verificamos que $w_{0}\cdot 1 = w_{0}$


$w_^{T}\cdot x_{i} = x_{i}\cdot w_^{T} = w_{0} $ 

de esta manera podemos usar la notación del procuto escalar para toda la regresión lineal

primero agregagmos `w0` al vector de pesos


```python
w_new = [w0] + w
```

el resultado de w_new es igual [7.17, 0.01 ,0.04 , 0.002]
>esto es similar a sumar entre listas


```python
[1]+[1,2,3]
```
> crearía una nueva lista [1,1,2,3]

y luego hacemos lo mismo con $x_{i}$ y calculamos del producto escalar con w_new


```python
def linear_regression(xi):
    xi = [1] + xi
    return dot(xi, w_new)
```
 y el resultado 
 
 ```python
 linear_regression(xi)
```

 debería ser el mismo `12.312`

Hasta acá hemos hecho regresión lineal de una característica específica de la matrix `X`

ahora vamos a implementar lo anterior para toda la matriz que en resumen sería una multiplicación entre la Matriz y el vector de pesos

tenemos

```python
w0 = 7.17
w = [0.01, 0.04, 0.002]
w_new = [w0] + w
```
y pongamos como ejemplo algunos vectores de características


$x_{1}$ = [1, 148, 24, 1385]
$x_{2}$ = [1, 132, 25, 2031]
$x_{10}$ = [1, 453, 11, 86]


agrupamos estos vectores a una lista y lo convertimos en una Matriz `X`


X = [$x_{1}$, $x_{2}$ , $x_{10}$ ]

```python
X = np.array(X)
```

y ahora hacemos el prodcuto escalar entre el vector de pesos y la matriz X 

```python
def linear_regression(X):
    return X.dot(w_new)
```

verificamos

```python
linear_regression(X)
```


El código y el conjunto de datos están disponibles en este [enlace](https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/chapter-02-car-price).
# 2.7 Entrenando el modelo de Regresion Lineal

## Notas

Obtener predicciones lo más cercanas posible a los valores objetivo $y$ requiere el cálculo de ponderaciones a partir de la ecuación general de Regresion Lineal (RL). La matriz de características no tiene inversa porque no es cuadrada, por lo que se requiere obtener una solución aproximada, que puede obtenerse mediante la **matriz de Gram** (multiplicación de la matriz de características ($X$) por su transpuesta ($X^T$)). El vector de ponderaciones o coeficientes $w$ obtenido con esta fórmula es la solución más cercana posible al sistema de RL.

Ecuación normal:

$w$ = $(X^TX)^{-1}X^Ty$

Donde:

$X^TX$ es la matriz de Gram


## ¿Cómo sigue?

Sabemos que la formula para la predicción del precio aplicando el modelo a la matriz `X` es esta

$g(X) = X.w$


y necesitamos aproximarnos a `y`

$X.w ≈ y$

para encontrar la solución mas cercana necesitamos encontrar  `w`, resolver ese sistema

Diagmos que $X$ es invertible, es decir que existe su matriz inversa $X^{-1}$ y se la aplicamos a las 2 partes de la ecuación

$X^{-1}\cdot X \cdot w ≈ $y\cdot X^{-1}$

y simplificando nos queda

$X = X^{-1}\cdot y$ 

Generalmente la matriz `X` es rectangular por tanto la inversa no existe.

Lo que podemos hacer es encontrar una aproximación a la solución multiplicando ambos términos de la ecuación por la matriz Transpuesta de `X` $X^{T}$

$X^{T}\cdot X \cdot w ≈ $y\cdot X^{T}$

esta parte suelen llamarla Matriz de Gramm

$X^{T}\cdot X \cdot w$

para la cual la inversa si existe por que siempre da como resultado una matriz cuadrada, condición necesaria para que exista la matriz inversa

entonces ahora si podemos multiplicar cada término por la inversa 

$(X^{T}\cdot X)^{-1}\cdot X^{T}\cdot X \cdot w ≈ (X^{T}\cdot X)^{-1}\cdot y\cdot X^{T}$

simplificando quedaría

$w ≈ (X^{T}\cdot X)^{-1}\cdot y\cdot X^{T}$

como $I.w$ da siempre $w$ colocamos solo `w`

y de esta manera es como podemos encontrar `w`

entendamos que sucede

```python
X = [
    [1, 148, 24, 1385],
    [1, 132, 25, 2031],
    [1, 453, 11, 86],
    [1, 158, 24, 185],
    [1, 172, 25, 201],
    [1, 413, 11, 86],
    [1, 38, 54, 185],
    [1, 142, 25, 431],
    [1, 453, 31, 86]
]

X = np.array(X)
```

ahora le agregamos el termino de sesgo a la matriz. Primero generamos un vector con la cantidad de filas de la matriz


```python
ones = np.ones(X.shape[0])
```

y luego agregamos el vector a la matriz


```python
X = np.column_stack([ones, X])
```

para ver el resultado podemos hacer

```python
list(np.column_stack([ones, X]))
```

ahora encontramos su Matriz de Gram


```python
XTX = x.T.dot(X)
```

encontramos su inversa


```python
XTX_inv = np.linalg.inv(XTX)
```

para ver su matriz Identidad hacemos


```python
XTX.dot(XTX_inv).round(1)
```


usamos un vector `y`, con valores altos para evitar resultado con números negativos y si aún asi los diera debemos entenderlo que para algunas características las relación con el precio es inversa, ejemplo, mientras mas años de antiguedad menor precio


```python
y=[10000,  20000, 15000, 25000, 10000,  20000, 15000, 25000, 12000]
```

y lo multiplicamos por `y` para obtener todos los pesos y lo guardamos en w_full


```python
w_full = XTX_inv.dot(X.T).dot(y)
```

podemos descomponerlo

```python
w0 = w_full[0] # serían todos los terminos sesgos
w = w_full[1:] # desde la segunda columnas hasta el final
```
 y estos son los coheficientes para nuestra Regresión Lineal

 ```python
 w0, w
 ```

Ahora si podemos poner todo en la función


```python
def train_linear_regression(X,y)
    ones = np.ones(X.shape[0])
    X = np.column_stack([ones, X])

    XTX = x.T.dot(X)
    XTX_inv = np.linalg.inv(XTX)
    w_full = XTX_inv.dot(X.T).dot(y)

    return w_full[0], w_full[1:]
```

probamos

```python
train_linear_regression(X,y)
```



El código completo de este proyecto está disponible en [este cuaderno Jupyter](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb).
